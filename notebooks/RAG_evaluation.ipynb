{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Alonmar\\Documents\\pruebas\\public-policy-evaluation-assistant\\venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "\n",
    "HUGGINGFACE_API = os.getenv(\"HUGGINGFACE_API\")\n",
    "model_transformer = \"all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "The file `RAG_EVALUATION.ipynb` is a Jupyter Notebook that contains code to evaluate a Retrieval-Augmented Generation (RAG) system. Below is a summary of the main sections and functions of the notebook:\n",
    "\n",
    "1. **Module Import**:\n",
    "    - Various libraries are imported, including `sentence_transformers`, `elasticsearch`, `dotenv`, `transformers`, `json`, `os`, `tqdm`, `time`, `torch`, and `pandas`.\n",
    "\n",
    "2. **Environment Variable Loading**:\n",
    "    - An environment variable `HUGGINGFACE_API` is loaded, and the model `model_transformer` is defined as `\"all-MiniLM-L6-v2\"`.\n",
    "\n",
    "3. **Utility Functions**:\n",
    "    - `read_json(file_path)`: Reads a JSON file and returns its content.\n",
    "    - `load_mode(model_name)`: Loads a `SentenceTransformer` model.\n",
    "    - `fetch_documents()`: Retrieves documents from a specific folder.\n",
    "    - `setup_elasticsearch(index_name, model, url_es)`: Sets up an Elasticsearch index.\n",
    "    - `index_documents(es_client, documents, model, index_name)`: Indexes documents in Elasticsearch.\n",
    "    - `init_elasticsearch(model_name, index_name)`: Initializes Elasticsearch with a model and an index.\n",
    "\n",
    "4. **Elasticsearch Initialization**:\n",
    "    - `init_elasticsearch` is called to set up and index documents in Elasticsearch.\n",
    "\n",
    "5. **Loading Verification Data**:\n",
    "    - A CSV file with verification data is loaded and converted into a list of dictionaries.\n",
    "\n",
    "6. **Loading Generation Models**:\n",
    "    - `load_model_generation(name_hf_model)`: Loads a text generation model from Hugging Face.\n",
    "\n",
    "7. **KNN Search in Elasticsearch**:\n",
    "    - `elastic_search_knn(field, vector, index_name)`: Performs a KNN search in Elasticsearch.\n",
    "\n",
    "8. **Prompt Construction and Response Generation**:\n",
    "    - `build_prompt(query, search_results, template)`: Constructs a prompt for text generation.\n",
    "    - `llm(prompt, pipe_generation)`: Generates a response using a language model.\n",
    "\n",
    "9. **RAG Function**:\n",
    "    - `rag(query, model, pipe_generation, template)`: Performs a RAG query and generates a response.\n",
    "\n",
    "10. **RAG System Evaluation**:\n",
    "     - `evalation_rag(ground_truth)`: Evaluates the relevance of the responses generated by the RAG system.\n",
    "\n",
    "11. **Running Evaluations**:\n",
    "     - Evaluations of the RAG system are performed using different text generation models.\n",
    "\n",
    "The notebook is designed to evaluate the relevance of the responses generated by a RAG system in the context of housing policies, using Elasticsearch for document retrieval and language models for response generation.\n",
    "1. **Module Import**:\n",
    "    - Various libraries are imported, including `sentence_transformers`, `elasticsearch`, `dotenv`, `transformers`, `json`, `os`, `tqdm`, `time`, `torch`, and `pandas`.\n",
    "\n",
    "2. **Environment Variable Loading**:\n",
    "    - An environment variable `HUGGINGFACE_API` is loaded, and the model `model_transformer` is defined as `\"all-MiniLM-L6-v2\"`.\n",
    "\n",
    "3. **Utility Functions**:\n",
    "    - `read_json(file_path)`: Reads a JSON file and returns its content.\n",
    "    - `load_mode(model_name)`: Loads a `SentenceTransformer` model.\n",
    "    - `fetch_documents()`: Retrieves documents from a specific folder.\n",
    "    - `setup_elasticsearch(index_name, model, url_es)`: Sets up an Elasticsearch index.\n",
    "    - `index_documents(es_client, documents, model, index_name)`: Indexes documents in Elasticsearch.\n",
    "    - `init_elasticsearch(model_name, index_name)`: Initializes Elasticsearch with a model and an index.\n",
    "\n",
    "4. **Elasticsearch Initialization**:\n",
    "    - `init_elasticsearch` is called to set up and index documents in Elasticsearch.\n",
    "\n",
    "5. **Loading Verification Data**:\n",
    "    - A CSV file with verification data is loaded and converted into a list of dictionaries.\n",
    "\n",
    "6. **Loading Generation Models**:\n",
    "    - `load_model_generation(name_hf_model)`: Loads a text generation model from Hugging Face.\n",
    "\n",
    "7. **KNN Search in Elasticsearch**:\n",
    "    - `elastic_search_knn(field, vector, index_name)`: Performs a KNN search in Elasticsearch.\n",
    "\n",
    "8. **Prompt Construction and Response Generation**:\n",
    "    - `build_prompt(query, search_results, template)`: Constructs a prompt for text generation.\n",
    "    - `llm(prompt, pipe_generation)`: Generates a response using a language model.\n",
    "\n",
    "9. **RAG Function**:\n",
    "    - `rag(query, model, pipe_generation, template)`: Performs a RAG query and generates a response.\n",
    "\n",
    "10. **RAG System Evaluation**:\n",
    "     - `evalation_rag(ground_truth)`: Evaluates the relevance of the responses generated by the RAG system.\n",
    "\n",
    "11. **Running Evaluations**:\n",
    "     - Evaluations of the RAG system are performed using different text generation models.\n",
    "\n",
    "The notebook is designed to evaluate the relevance of the responses generated by a RAG system in the context of housing policies, using Elasticsearch for document retrieval and language models for response generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_mode(model_name):\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "def fetch_documents():\n",
    "    print(\"Fetching documents...\")\n",
    "\n",
    "    directory_path = \"../json_data\"\n",
    "\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory_path)\n",
    "\n",
    "    documents = []\n",
    "    for file in files:\n",
    "        print(f\"Reading file: {file}\")\n",
    "        data = read_json(f\"{directory_path}/{file}\")\n",
    "        documents.extend(data)\n",
    "        print(f\"Fetched {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "def setup_elasticsearch(index_name, model, url_es=\"http://localhost:9200\"):\n",
    "    print(\"Setting up Elasticsearch...\")\n",
    "    es_client = Elasticsearch(url_es)\n",
    "\n",
    "    index_settings = {\n",
    "        \"settings\": {\"number_of_shards\": 1, \"number_of_replicas\": 0},\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"doc_id\": {\"type\": \"keyword\"},\n",
    "                \"page_num\": {\"type\": \"integer\"},\n",
    "                \"chunk_id\": {\"type\": \"keyword\"},\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"text_vector\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": model.get_sentence_embedding_dimension(),\n",
    "                    \"index\": True,\n",
    "                    \"similarity\": \"cosine\",\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    es_client.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "    es_client.indices.create(index=index_name, body=index_settings)\n",
    "    print(f\"Elasticsearch index '{index_name}' created\")\n",
    "    return es_client\n",
    "\n",
    "\n",
    "def index_documents(es_client, documents, model, index_name):\n",
    "    print(\"Indexing documents...\")\n",
    "    for doc in tqdm(documents):\n",
    "        doc[\"text_vector\"] = model.encode(doc[\"text\"]).tolist()\n",
    "        es_client.index(index=index_name, document=doc)\n",
    "    print(f\"Indexed {len(documents)} documents\")\n",
    "\n",
    "\n",
    "def init_elasticsearch(model_name, index_name):\n",
    "    model = load_mode(model_name)\n",
    "    documents = fetch_documents()\n",
    "    es_client = setup_elasticsearch(index_name, model)\n",
    "    index_documents(es_client, documents, model, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: all-MiniLM-L6-v2\n",
      "Fetching documents...\n",
      "Reading file: Cityphilia-and-cityphobia--A-multi-scalar-search-for_2024_Journal-of-Urban-M.json\n",
      "Fetched 60 documents\n",
      "Reading file: How-do-local-governments-respond-to-central-mandate-in-affo_2024_Journal-of-.json\n",
      "Fetched 113 documents\n",
      "Reading file: Inclusive-cities--Less-crime-requires-more-lo_2024_Journal-of-Urban-Manageme.json\n",
      "Fetched 118 documents\n",
      "Reading file: sideris_gonzales_ong.json\n",
      "Fetched 171 documents\n",
      "Reading file: The_High_Cost_of_Free_Parking.json\n",
      "Fetched 190 documents\n",
      "Setting up Elasticsearch...\n",
      "Elasticsearch index 'esearchtext_model_all-minilm-l6-v2' created\n",
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [00:09<00:00, 19.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 190 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "init_elasticsearch(model_transformer, \"esearchtext_model_all-minilm-l6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(\"../data_output/ground-truth-retrieval.csv\")\n",
    "ground_truth = ground_truth.to_dict(orient=\"records\")\n",
    "ground_truth = ground_truth[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_generation(name_hf_model):\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        name_hf_model,\n",
    "        device_map=device,\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        token=HUGGINGFACE_API,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        name_hf_model,\n",
    "        token=HUGGINGFACE_API,\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search_knn(\n",
    "    field,\n",
    "    vector,\n",
    "    # course,\n",
    "    index_name,\n",
    "):\n",
    "    es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "    knn = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 10000,\n",
    "        # \"filter\": {\"term\": {\"course\": course}},\n",
    "    }\n",
    "\n",
    "    search_query = {\n",
    "        \"knn\": knn,\n",
    "        \"_source\": [\"doc_id\", \"page_num\", \"chunk_id\", \"text\"],\n",
    "    }\n",
    "\n",
    "    es_results = es_client.search(index=index_name, body=search_query)\n",
    "\n",
    "    return [hit[\"_source\"] for hit in es_results[\"hits\"][\"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results, template):\n",
    "    prompt_template = \"\"\"\n",
    "{template}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        [f\"doc_id: {doc['doc_id']}\\nanswer: {doc['text']}\" for doc in search_results]\n",
    "    )\n",
    "    return prompt_template.format(\n",
    "        question=query, context=context, template=template\n",
    "    ).strip()\n",
    "\n",
    "\n",
    "def llm(prompt, pipe_generation):\n",
    "    # return {\"answer\": \"test\", \"time\": 0.0}\n",
    "    start_time = time.time()\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    eos_token_id = pipe_generation.tokenizer.eos_token_id\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"return_full_text\": False,\n",
    "        # \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "        \"pad_token_id\": eos_token_id,\n",
    "    }\n",
    "\n",
    "    output = pipe_generation(messages, **generation_args)\n",
    "\n",
    "    answer = output[0][\"generated_text\"].strip()\n",
    "\n",
    "    end_time = time.time()\n",
    "    response_time = end_time - start_time\n",
    "\n",
    "    return {\"answer\": answer, \"time\": response_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, model, pipe_generation, template):\n",
    "    search_results = elastic_search_knn(\n",
    "        \"text_vector\", model.encode(query), \"esearchtext_model_all-minilm-l6-v2\"\n",
    "    )\n",
    "    prompt = build_prompt(query, search_results, template)\n",
    "    return llm(prompt, pipe_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]d:\\Alonmar\\Documents\\pruebas\\public-policy-evaluation-assistant\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "d:\\Alonmar\\Documents\\pruebas\\public-policy-evaluation-assistant\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      " 40%|████      | 10/25 [02:01<03:41, 14.76s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 25/25 [04:40<00:00, 11.21s/it]\n"
     ]
    }
   ],
   "source": [
    "model = load_mode(model_transformer)\n",
    "pipe_generation = load_model_generation(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "template = \"\"\"As a housing policy expert advising policymakers, answer the QUESTION below using only the verified information provided in the CONTEXT. \n",
    "Maintain a neutral, factual tone, and avoid assumptions or extrapolations beyond the CONTEXT. \n",
    "Structure your response with a brief summary of pros and cons to support balanced decision-making, and keep the response not more that 30 words.\"\"\"\n",
    "\n",
    "for row in tqdm(ground_truth):\n",
    "    row[\"gen_answer\"] = rag(row[\"question\"], model, pipe_generation, template)[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'Cityphilia-and-cityphobia--A-multi-scalar-search-for_2024_Journal-of-Urban-M_1_1',\n",
       "  'question': 'How do you think the concept of cityphobia can be used to inform policy decisions aimed at reducing urban poverty and inequality, particularly in areas with high levels of social exclusion?',\n",
       "  'gen_answer': \"**Summary of Pros and Cons:**\\n\\nTo inform policy decisions aimed at reducing urban poverty and inequality, using the concept of cityphobia can be beneficial. On the one hand, cityphobia can highlight the importance of love and attachment to one's living environment, which can lead to more inclusive and sustainable urban development. On the other hand, cityphobia can also be used to justify the need for more commercial housing and social services, potentially exacerbating existing inequalities.\\n\\n**Balanced Decision-Making:**\\n\\n* Cityphobia can emphasize the importance of love and attachment to one's living environment, leading to more inclusive and sustainable urban development.\\n* However, cityphobia can also be used to justify the need for more commercial housing and social services, potentially exacerbating existing inequalities.\\n* A balanced approach would consider both the benefits and drawbacks of cityphobia, and prioritize inclusive and sustainable urban development that addresses the needs of all residents.\"},\n",
       " {'id': 'Cityphilia-and-cityphobia--A-multi-scalar-search-for_2024_Journal-of-Urban-M_1_1',\n",
       "  'question': \"What role do you believe the 'body' component of city love should play in the development of policies aimed at promoting physical and mental well-being in urban areas?\",\n",
       "  'gen_answer': '**Summary of Pros and Cons:**\\n\\n**Pros:**\\n\\n1. The study aims to develop an operational framework for understanding citizens\\' appreciation and attachment to their living environment, which is a crucial aspect of urban well-being.\\n2. The researchers use a novel concept of \"city love\" to assess the determinants of residents\\' well-being, which is a promising approach for a comprehensive understanding of urban well-being perceptions.\\n3. The study explores the relationship between the tangible facets of urban life (body) and the intangible aspects of the city (soul), which is a key aspect of the \"cityphilia\" –\"cityphobia\" dichotomy.\\n\\n**Cons:**\\n\\n1. The study relies on subjective survey-based data, which may be subject to biases and limitations.\\n2. The researchers do not provide a clear definition of \"city love\" or its components, which may limit the generalizability of the findings.\\n3. The study does not account for potential confounding variables, such as socio-economic factors, which may influence residents\\' well-being perceptions.'},\n",
       " {'id': 'Cityphilia-and-cityphobia--A-multi-scalar-search-for_2024_Journal-of-Urban-M_1_1',\n",
       "  'question': \"How do you think the'soul' component of city love, encompassing social cohesion and community engagement, can be leveraged to enhance the resilience of urban communities in the face of economic and social challenges?\",\n",
       "  'gen_answer': \"**Summary of Pros and Cons:**\\n\\nPros:\\n\\n* The study aims to understand the determinants of urban well-being perceptions among citizens, providing a comprehensive understanding of the factors that shape city love.\\n* The concept of 'city love' offers a promising avenue for acomprehensive understanding of the determinants of urban well-being perceptions among citizens.\\n* The study leverages the concept of 'cityphilia' and 'cityphobia' as metaphors for the spatial attraction and repulsion forces that shape local quality of life.\\n\\nCons:\\n\\n* The study relies on subjective survey-based and objective data, which may be subject to biases and limitations.\\n* The study focuses on a specific region (Flanders, Belgium) and may not be generalizable to other regions or cities.\\n* The study does not account for the complex interplay between different factors that shape city love, such as governance, economic factors, and social capital.\"},\n",
       " {'id': 'Cityphilia-and-cityphobia--A-multi-scalar-search-for_2024_Journal-of-Urban-M_1_2',\n",
       "  'question': \"How do you respond to the argument that the 'city love' framework, which emphasizes the importance of central place systems in providing well-being services, may be overly simplistic or neglects the complexities of individual experiences in urban and rural settings?\",\n",
       "  'gen_answer': \"**Summary of Pros and Cons:**\\n\\n**Pros:**\\n\\n1. The 'city love' framework provides a comprehensive understanding of the determinants of urban well-being perceptions among citizens.\\n2. The study highlights the importance of both tangible and intangible aspects of urban life in shaping individual experiences.\\n3. The framework offers a promising avenue for a comprehensive understanding of urban well-being perceptions among citizens.\\n\\n**Cons:**\\n\\n1. The study's focus on a specific region (Flanders, Belgium) may limit its generalizability to other urban and rural settings.\\n2. The use of subjective survey-based and objective data may be subject to biases and limitations.\\n3. The study's operationalization of the 'city love' framework may be open to interpretation and require further refinement.\"},\n",
       " {'id': 'Cityphilia-and-cityphobia--A-multi-scalar-search-for_2024_Journal-of-Urban-M_1_2',\n",
       "  'question': 'What implications do you see for urban policy and management in light of the findings that local attractiveness and interdependence shape the well-being of residents in diverse urban and rural settings?',\n",
       "  'gen_answer': 'Summary of pros and cons:\\n\\n**Pros:**\\n\\n* The study highlights the importance of local attractiveness and interdependence in shaping the well-being of residents in diverse urban and rural settings.\\n* The concept of \"city love\" provides a useful framework for understanding the complex relationship between citizens and their urban living environments.\\n* The study\\'s findings have implications for urban policy and management, suggesting that cities should prioritize local attractiveness and interdependence in their development and management strategies.\\n\\n**Cons:**\\n\\n* The study\\'s focus on a specific geographic region (the \"urban century\") may limit its generalizability to other contexts.\\n* The use of a single measure of well-being (happiness) may not capture the full range of factors that contribute to residents\\' satisfaction with their urban living environments.\\n* The study\\'s reliance on a single theoretical framework (the \"city love\" concept) may not account for other important factors that influence residents\\' well-being.'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalation_rag(ground_truth):\n",
    "    def build_prompt_template(question, gen_answer):\n",
    "        prompt_template = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system. \n",
    "You task is to analyse the relevance of de answer to the question and context provided.\n",
    "The answer try to repond like a housing policy expert.\n",
    "Based on the relevance of the answer, you have to classify it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "GENERATED ANSWER:\n",
    "{gen_answer}\n",
    "\n",
    "Return the output in a well-formed JSON format without code blocks.\n",
    "\n",
    "{{\n",
    "    RELEVANCE: \"NON_RELEVANT\" | \"RELEVANT\" | \"HIGHLY_RELEVANT\"\n",
    "    Explanation: \"[Provide a brief explanation of your decision]\"\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "        return prompt_template.format(question=question, gen_answer=gen_answer).strip()\n",
    "\n",
    "    status_list = []\n",
    "    for ground_truth_row in tqdm(ground_truth):\n",
    "        prompt = build_prompt_template(\n",
    "            ground_truth_row[\"question\"], ground_truth_row[\"gen_answer\"]\n",
    "        )\n",
    "        llm_output = llm(prompt, pipe_generation)[\"answer\"]\n",
    "        try:\n",
    "            evaluation_result = json.loads(llm_output)\n",
    "        except json.JSONDecodeError:\n",
    "            evaluation_result = {\n",
    "                \"RELEVANCE\": \"ERROR\",\n",
    "                \"Explanation\": \"failed to parse JSON\",\n",
    "            }\n",
    "\n",
    "        if (\n",
    "            \"RELEVANCE\" not in evaluation_result\n",
    "            or \"Explanation\" not in evaluation_result\n",
    "        ):\n",
    "            evaluation_result = {\"RELEVANCE\": \"ERROR\", \"Explanation\": \"bad JSON format\"}\n",
    "\n",
    "        status_list.append(llm_output)\n",
    "\n",
    "    full_load_json = []\n",
    "\n",
    "    for i in tqdm(status_list):\n",
    "        load_json = json.loads(i)\n",
    "\n",
    "        full_load_json.append(load_json)\n",
    "\n",
    "    relevance_df = pd.DataFrame(full_load_json)\n",
    "    return relevance_df.value_counts(\"RELEVANCE\") / relevance_df.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the RAG system whit meta-llama/Llama-3.2-1B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]d:\\Alonmar\\Documents\\pruebas\\public-policy-evaluation-assistant\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "d:\\Alonmar\\Documents\\pruebas\\public-policy-evaluation-assistant\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [01:32<00:00,  3.68s/it]\n",
      "100%|██████████| 25/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RELEVANCE\n",
       "NON_RELEVANT    100.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluation of the RAG system whit meta-llama/Llama-3.2-1B-Instruct\")\n",
    "evalation_rag(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_mode(model_transformer)\n",
    "pipe_generation = load_model_generation(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "template = \"\"\"Answer the QUESTION below using only the verified information provided in the CONTEXT. \n",
    "Maintain a neutral, factual tone, and avoid assumptions or extrapolations beyond the CONTEXT. \n",
    "Structure your response with a brief summary of pros and cons to support balanced decision-making, and keep the response not more that 30 words.\"\"\"\n",
    "\n",
    "for row in tqdm(ground_truth):\n",
    "    row[\"gen_answer\"] = rag(row[\"question\"], model, pipe_generation, template)[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation of the RAG system whit facebook/bart-large-cnn\")\n",
    "evalation_rag(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 24989.90it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RELEVANCE\n",
       "NON_RELEVANT    100.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
